# Ex.No.4-EXP 4 Generate the Prompt and eveluate that for following prompt types - Comparative Analysis Prompt Experiential Perspective Prompt - Everyday Functioning Prompts - Universal Prompt Structures Prompt Refinements- Prompt Size Limitations.
### DATE:                                                                            
### REGISTER NUMBER : 
### Aim: To write the prompt for the following prompt types and ompare that with differenet AI tools and evaluate that using any one evaluation method (eg. Rubrics). 1.Comparative Analysis Prompt 2. Experiential Perspective Prompt 3. Everyday Functioning Prompts 4. Universal Prompt Structures Prompt Refinements 5. Prompt Size Limitations design an AI 

### Explanation - Any one use case from Unit 5 and write the prompt for that with the unit 2 Prompt types given above.

Procedure:
1.	Define the Scenario and Use Case:
Scenario:
The manufacturing industry is looking to reduce manual monitoring and increase efficiency through automation. The system will utilize IoT devices and embedded controllers to automate equipment, monitor performance, and enable predictive maintenance. The goal is to streamline the production process, minimize downtime, and enhance energy efficiency.
Target Audience:
Manufacturing companies, specifically in sectors like automotive, electronics, and food processing, where automation can significantly improve productivity.

Main Objectives:

•	Improve production efficiency by 30%.
•	Minimize machinery downtime with predictive maintenance.
•	Enable real-time monitoring and remote control of manufacturing systems.
•	Reduce energy consumption by optimizing processes.
 
2.	Identify Prompt Patterns for Each Design Aspect:
Idea Generation Prompts:

Comparative Analysis Prompt
Definition: Asking for a comparison between two or more concepts, often highlighting similarities and differences.
•	"Compare and contrast deep learning and machine learning."
•	"How do renewable and non-renewable energy sources differ?"

Experiential Perspective Prompt:
> 	“As a software engineer, what are the benefits of using cloud computing in your projects?”
>  “From a student’s perspective, describe the challenges of online learning.”

Everyday Functioning Prompts:
*	  “Describe the role of machine learning in everyday apps like YouTube or Netflix."
*	 	“How is encryption used in daily life when sending WhatsApp messages?”

Universal Prompt Structures:
^ 	“Explain [concept] in simple terms with an example.”
^  “Explain the process of [task] step by step.”

Prompt Size Limitations:
&  	“Translate this 5000-word essay into Tamil.”
→ Manageable chunks: “Translate the first 1000 words into Tamil, then continue.”
&  “Give me 50 examples of AI applications.”
→ Adjust for size: “Give me 10 AI applications at a time.”

##output
Scenario-Based Report: Exploring Prompting Techniques in Generative AI
Scenario Context
You are part of an AI research and training team working for an educational technology company, EduThink AI, which develops AI-driven learning tools for students and educators. Your task is to explore how different prompting strategies can improve the performance of a generative AI system when creating student-friendly study materials and analytical reports.

1. Comparative Analysis Prompt
Scenario
EduThink AI aims to evaluate how different AI models summarize academic content. The team uses a Comparative Analysis Prompt to analyze summaries generated by ChatGPT, Gemini, and Claude.
Example Prompt
“Compare the summarization styles of ChatGPT, Gemini, and Claude for a 500-word article on ‘The Basics of Blockchain Technology’. Analyze clarity, technical accuracy, and accessibility for undergraduate learners.”
Outcome
ChatGPT: Produced structured and pedagogical summaries.
Gemini: Focused on high-level abstraction, missing minor details.
Claude: Balanced technical and conceptual explanations.
Insight
Comparative prompts enhance evaluation skills of AI, promoting multi-model reasoning and cross-platform benchmarking — valuable for AI audit reports and education analytics.


2. Experiential Perspective Prompt
Scenario
The marketing department needs an AI-generated report reflecting human-like insight into student experience while using AI tutors.
Example Prompt
“From the perspective of a college student preparing for final exams, describe how an AI tutor powered by generative AI improves learning engagement and stress management.”
Outcome
The AI adopted a first-person narrative, integrating emotional and cognitive experiences, making the report empathetic and relatable.
Insight
Experiential prompts bridge AI outputs and human context, improving personalization in educational content creation.

3. Everyday Functioning Prompts
Scenario
The operations team wants AI assistance for daily task automation like scheduling, feedback analysis, and progress tracking.
Example Prompt
“Summarize today’s student feedback and list three key improvements for tomorrow’s class schedule.”
Outcome
AI generated quick actionable summaries, displaying context awareness and task adaptability.

Insight
Everyday functioning prompts enhance practical utility, embedding AI seamlessly into administrative and teaching routines.

4. Universal Prompt Structures
Scenario
EduThink wants consistent prompt formats usable across multiple departments (education, marketing, R&D).
Universal Prompt Framework
[Role] + [Task] + [Context] + [Output Format] + [Constraints]
Example
“As an educational analyst, summarize the key advantages of gamified learning in 100 words, focusing on student engagement.”
Outcome
Uniform, predictable responses with professional tone and format, regardless of task type.
Insight
Universal structures improve prompt reusability, scalability, and training efficiency in multi-department AI integration.

5. Prompt Refinements
Scenario
Initial AI outputs were too generic. The team refined prompts iteratively to increase depth and specificity.


Example

Initial Prompt:
“Explain blockchain simply.”
Refined Prompt:
“Explain blockchain technology to a 12th-grade commerce student using examples from digital payments.”
Outcome
Refined prompts led to context-sensitive, audience-appropriate outputs.
Insight
Prompt refinement enhances clarity, target accuracy, and domain alignment, especially for education and training applications.

6. Prompt Size Limitations
Scenario
When generating detailed research comparisons, AI models struggled with overly long prompts (exceeding token limits).
Example Issue
A 3,000-word input on “AI in healthcare” caused truncation and loss of focus.
Solution
The team used chunked prompts — dividing the input into sections (definition, application, ethics).
Insight
Managing prompt size ensures efficiency, memory optimization, and context retention during report generation.

7. Conclusion
Technique	Core Benefit	Use Case	Challenge
Comparative Analysis Prompt	Model benchmarking	Research evaluation	Requires structured metrics
Experiential Perspective Prompt	Human-like empathy	Storytelling, engagement	Subjectivity may vary
Everyday Functioning Prompt	Task automation	Daily reports, summaries	Needs frequent updates
Universal Prompt Structures	Standardization	Multi-team workflows	May reduce creativity
Prompt Refinement	Output accuracy	Educational content	Time-intensive iteration
Prompt Size Limitation	Performance stability	Long-form generation	Requires chunking strategy

Final Insight
Using a scenario-based prompting approach allows organizations like EduThink AI to create adaptive, efficient, and human-centered AI systems.
Combining structured, experiential, and functional prompts ensures that AI outputs are accurate, relatable, and operationally valuable, supporting both academic excellence and organizational productivity.


# Result: The various types of Prompts are executed successfully with generated the report.
